{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import special_ortho_group\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import itertools\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, roc_auc_score, confusion_matrix\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from From_clusters_to_couples import CreateCouples\n",
    "from Transfomations import apply_translation, apply_randomrotations\n",
    "from PCT_definition import sample_and_group, farthest_point_sample, index_points, square_distance, Local_op, SA_Layer, StackedAttention\n",
    "from Model_7_features import use_GPU, PairModel1, Branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=999\n",
    "os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.load(\"C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\pair_dataset\\\\dataset_1024_AB\\\\train_pair_dataset_REG.pt\")\n",
    "\n",
    "with open('C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\PairModel\\\\val_subset_random.pkl', 'rb') as file:\n",
    "    val_randomized = pickle.load(file)\n",
    "\n",
    "with open('C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\PairModel\\\\test_subset_random.pkl', 'rb') as file:\n",
    "    test_randomized = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unroll the pairs from the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_couples = CreateCouples(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify positive and negative couples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_couples_0 = [item for item in train_couples if item[2] == 0]\n",
    "train_couples_1 = [item for item in train_couples if item[2] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the validation dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_basic = DataLoader(val_randomized, batch_size=16) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4080 is available and being used\n"
     ]
    }
   ],
   "source": [
    "device=use_GPU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PairModel1(\n",
       "  (branch1): Branch(\n",
       "    (conv1): Conv1d(7, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (conv2): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (gather_local_0): Local_op(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (gather_local_1): Local_op(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pt_last): StackedAttention(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (sa1): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (sa2): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (sa3): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (sa4): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "    (conv_fuse): Sequential(\n",
       "      (0): Conv1d(1280, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (branch2): Branch(\n",
       "    (conv1): Conv1d(7, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (conv2): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (gather_local_0): Local_op(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (gather_local_1): Local_op(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pt_last): StackedAttention(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (sa1): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (sa2): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (sa3): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (sa4): SA_Layer(\n",
       "        (q_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (k_conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (v_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (trans_conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (after_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "    (conv_fuse): Sequential(\n",
       "      (0): Conv1d(1280, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (dp1): Dropout(p=0.5, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (linear1): Linear(in_features=2048, out_features=512, bias=False)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dp2): Dropout(p=0.5, inplace=False)\n",
       "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dp3): Dropout(p=0.5, inplace=False)\n",
       "  (linear3): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PairModel1().to(device)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only when using pre-trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_stored = torch.load(r'C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Weights\\\\Main_epoch116.pt')\n",
    "model.load_state_dict(W_stored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters and prepare Weights & Biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msottile124\u001b[0m (\u001b[33mpair_fragments\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Alessandro\\Desktop\\Tesi\\PairModel\\Files_finali\\wandb\\run-20240229_161709-opyguu88</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pair_fragments/train/runs/opyguu88' target=\"_blank\">feasible-glitter-6</a></strong> to <a href='https://wandb.ai/pair_fragments/train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pair_fragments/train' target=\"_blank\">https://wandb.ai/pair_fragments/train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pair_fragments/train/runs/opyguu88' target=\"_blank\">https://wandb.ai/pair_fragments/train/runs/opyguu88</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "      project=\"train\", \n",
    "      notes = \"seven_features\",\n",
    "      config={\n",
    "      \"learning_rate\": 0.00005,\n",
    "      \"architecture\": \"Model2_mod\",\n",
    "      \"epochs\": 20,\n",
    "      \"weight_decay\": 0.0001,\n",
    "      \"W_crossentropy\":1,\n",
    "      \"couples_per_epoch\": 10000,\n",
    "      \"seed\": seed # the seed defined at the beginning\n",
    "      })\n",
    "      \n",
    "config = wandb.config\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "num_epochs = config.epochs\n",
    "best_val_accuracy = 0.0 \n",
    "\n",
    "\n",
    "# Sets the path where the model weights will be stored.\n",
    "# make sure to enter the right destination folder, otherwise the training cycle will stop by not finding the folder\n",
    "checkpoint_dir = r'C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\PairModel\\\\Check_points'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training cycle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_interval = 1\n",
    "epoch_number = 0  \n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    random.shuffle(train_couples_0)\n",
    "    random.shuffle(train_couples_1)\n",
    "\n",
    "    # 5000 positive pairs and 5000 negative pairs are used\n",
    "    balanced_train_list = []\n",
    "    couples_per_epoch = config.couples_per_epoch\n",
    "    \n",
    "    sampled_couples_0 = random.sample(train_couples_0, couples_per_epoch // 2)\n",
    "    sampled_couples_1 = random.sample(train_couples_1, couples_per_epoch // 2)\n",
    "\n",
    "    balanced_train_list.extend(sampled_couples_0)\n",
    "    balanced_train_list.extend(sampled_couples_1)\n",
    "        \n",
    "    random.shuffle(balanced_train_list)\n",
    "    \n",
    "    train_loader = DataLoader(balanced_train_list, batch_size=16,shuffle=True) \n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "\n",
    "    ###########\n",
    "    ## Train ##\n",
    "    ###########\n",
    "\n",
    "\n",
    "    for batch_data in progress_bar:\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        frags_a, frags_b, labels = batch_data\n",
    "        \n",
    "        # Random rotations\n",
    "        frags_a = apply_randomrotations(frags_a)\n",
    "        frags_b = apply_randomrotations(frags_b)\n",
    "\n",
    "        # Translations in the origin\n",
    "        frags_a = apply_translation(frags_a)\n",
    "        frags_b = apply_translation(frags_b)\n",
    "\n",
    "        frags_a = frags_a.double().to(device)\n",
    "        frags_b = frags_b.double().to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Model output\n",
    "        outputs = model(frags_a, frags_b)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        progress_bar.set_postfix({'Loss': loss.item(), 'Accuracy': correct_predictions / total_samples})\n",
    "        \n",
    "    # Metrics computation\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    train_loss = total_loss/len(train_loader)\n",
    "\n",
    "    metrics_train = {\"train_loss\": train_loss, \n",
    "                       \"accuracy\": accuracy,\n",
    "                       \"f1\":f1}\n",
    "    \n",
    "    # Upload results to wandb\n",
    "    wandb.log(metrics_train) \n",
    "\n",
    "\n",
    "\n",
    "    ###############\n",
    "    ## Inference ##\n",
    "    ###############\n",
    "\n",
    "    model.eval()  \n",
    "    \n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "    y_true_val = []\n",
    "    y_pred_val = []\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_loader_basic:\n",
    "            val_frags_a, val_frags_b, val_labels = val_batch\n",
    "            \n",
    "            # Translations in the origin\n",
    "            val_frags_a = apply_translation(val_frags_a)\n",
    "            val_frags_b = apply_translation(val_frags_b)\n",
    "\n",
    "            val_frags_a = val_frags_a.double().to(device)\n",
    "            val_frags_b = val_frags_b.double().to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "\n",
    "            # Model output\n",
    "            val_outputs = model(val_frags_a, val_frags_b)\n",
    "\n",
    "            val_loss += criterion(val_outputs, val_labels).item()\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_total_samples += val_labels.size(0)\n",
    "            val_correct_predictions += (val_predicted == val_labels).sum().item()\n",
    "            y_true_val.extend(val_labels.cpu().numpy())\n",
    "            y_pred_val.extend(val_predicted.cpu().numpy())\n",
    "\n",
    "    # Metrics computation\n",
    "    f1_val = f1_score(y_true_val, y_pred_val, average='weighted')        \n",
    "    val_accuracy = val_correct_predictions / val_total_samples\n",
    "    val_loss /= len(val_loader_basic)\n",
    "    val_metrics = {\"val_loss\": val_loss, \n",
    "                       \"val_accuracy\": val_accuracy,\n",
    "                       \"f1_val\": f1_val}\n",
    "    \n",
    "    # Upload results to wandb\n",
    "    wandb.log(val_metrics)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Training Accuracy: {accuracy:.4f}, F1_train:{f1:.4f} ',\n",
    "    f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, F1_val : {f1_val:.4f}')\n",
    "\n",
    "\n",
    "    # Store the results    \n",
    "    current_time = datetime.datetime.now()\n",
    "    checkpoint_name = f\"{current_time.strftime('%m%d_%H%M%S')}_{epoch + 1}.pt\"    \n",
    "    # make sure to enter the right destination folder, otherwise the training cycle will stop by not finding the folder\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name) \n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    wandb.run.log_artifact(checkpoint_path,name=str(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained weight loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_stored = torch.load(r'C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Weights\\\\Main_epoch116.pt')\n",
    "model.load_state_dict(W_stored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Test dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_basic = DataLoader(test_randomized, batch_size=16) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005, weight_decay=0.0001)\n",
    "num_epochs = 1\n",
    "best_val_accuracy = 0.0 \n",
    "# Sets the path where the model weights will be stored.\n",
    "checkpoint_dir = r'C:\\\\Users\\\\Alessandro\\\\Desktop\\\\Tesi\\\\PairModel\\\\Check_points'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference cycle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  \n",
    "progress_bar_val = tqdm(test_loader_basic, desc=f'Validation', leave=False)    \n",
    "val_loss = 0.0\n",
    "val_correct_predictions = 0\n",
    "val_total_samples = 0\n",
    "\n",
    "y_true_val = []\n",
    "y_pred_val = []\n",
    "y_scores_val = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_batch in progress_bar_val:\n",
    "        val_frags_a, val_frags_b, val_labels = val_batch\n",
    "        \n",
    "        # Translations in the origin\n",
    "        val_frags_a = apply_translation(val_frags_a)\n",
    "        val_frags_b = apply_translation(val_frags_b)\n",
    "        \n",
    "        val_frags_a = val_frags_a.double().to(device)\n",
    "        val_frags_b = val_frags_b.double().to(device)\n",
    "\n",
    "        val_labels = val_labels.to(device)\n",
    "            \n",
    "        val_outputs = model(val_frags_a, val_frags_b)\n",
    "        \n",
    "        val_outputs_probs = softmax(val_outputs, dim=1)\n",
    "        val_loss += criterion(val_outputs, val_labels).item()\n",
    "        _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "        val_total_samples += val_labels.size(0)\n",
    "        val_correct_predictions += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "        y_true_val.extend(val_labels.cpu().numpy())\n",
    "        y_pred_val.extend(val_predicted.cpu().numpy())\n",
    "        y_scores_val.extend(val_outputs_probs.cpu().numpy()[:, 1])  \n",
    "        \n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true_val, y_scores_val)\n",
    "\n",
    "# AUC\n",
    "roc_auc = roc_auc_score(y_true_val, y_scores_val)\n",
    "\n",
    "# F1-score, Accuracy, Val_loss\n",
    "f1_val = f1_score(y_true_val, y_pred_val, average='weighted') \n",
    "val_accuracy = val_correct_predictions / val_total_samples\n",
    "val_loss /= len(test_loader_basic)\n",
    "\n",
    "# Print\n",
    "print(\"Test Accuracy:\", val_accuracy)\n",
    "print(\"Test F1 Score:\", f1_val)\n",
    "print(\"Test AUC:\", roc_auc)\n",
    "print(\"Test Loss\", val_loss)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true_val, y_pred_val)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
